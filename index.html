<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge Graph-Based RAG</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge Graph-Based RAG</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/kavanavenkatesh/">Kavana Venkatesh</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://yusufdalva.github.io/">Yusuf Dalva</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://isminoula.github.io/">Ismini Lourentzou</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://pinguar.org/">Pinar Yanardag</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Virginia Tech,</span>
            <span class="author-block"><sup>2</sup>University of Illinois Urbana-Champaign</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-widescreen">
    <div class="hero-body">
      <!-- TL;DR Section -->
      <h2 class="title is-5" style="font-weight: 400; text-align: justify; margin-bottom: 20px; color: #444;">
        <strong>TL;DR</strong> We propose a training-free approach that leverages knowledge graph-based Retrieval Augmented Generation to 
        enhance image generation and editing of text-to-image diffusion models. We also introduce a novel RAG context-guided self-correction mechanism. 
        The approach enables generation of contextually and narratively accurate images for
        complex, domain-specific scenarios that standard T2I models struggle with, using simple high-level user prompts.
      </h2>
      <!-- Teaser image -->
      <img id="teaser-image" src="./static/images/teaser.jpg" alt="Teaser Image" style="width: 120%; height: auto;">
      <h2 class="subtitle teaser-caption" style="font-size: 1rem; text-align: justify;">
      <span class="dnerf">Context Canvas</span> significantly improves image generation for domain-specific, complex characters that T2I models might
        otherwise struggle with (top left). Additionally, our method enables disentangled image editing by extracting precise item descriptions,
        such as transforming a simple “add a sword” prompt into a fire sword for the character Jambavan. It also adeptly captures relationships
        by utilizing the graph, for instance, generating Jambavan’s daughter from a simple “with his daughter” prompt, without needing explicit
        details about her appearance (bottom middle). Moreover, we introduce a novel RAG-based self-correction technique that further refines
        images to improve visual and narrative accuracy (right).
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce a novel approach to enhance the capabilities of text-to-image models by incorporating Retrieval-Augmented Generation 
            with a knowledge graph. Our system dynamically retrieves detailed character information
            and relational data from the knowledge graph, enabling the
            generation of visually accurate and contextually rich images. Furthermore, we propose a self-correcting mechanism 
            within Stable Diffusion models to ensure consistency
            and fidelity in visual outputs, leveraging the rich context
            from the graph to guide corrections. To our knowledge, <span class="dnerf">Context Canvas</span> represents 
            the first application of graph-based RAG in enhancing
            T2I models, representing a significant advancement for producing high-fidelity, context-aware multi-faceted images.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</section>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <div class="columns is-centered">
        <div class="column">
          <!-- Paper Method Diagram -->
          <h2 class="title is-3 has-text-centered">Method</h2>
          <div class="publication-diagram" style="text-align: center;">
            <img id="method-diagram" src="./static/images/method-diagram.jpg" alt="Paper Method Diagram" style="width: 100%; height: auto; margin-top: 20px;">
            <p class="teaser-caption" style="font-size: 1rem; line-height: 1.5; text-align: justify; margin-top: 10px; margin-bottom: 20px; color: #444;">
              Context Canvas introduces a novel knowledge graph-based Retrieval-Augmented Generation (RAG) framework to 
              enhance text-to-image diffusion models for context-driven generation and editing of images with complex, 
              domain-specific concepts. The framework includes three key stages: 
              knowledge graph-driven image generation, context-aware image editing, and a novel self-correction mechanism. 
              In the first stage, user prompts are enriched with detailed character attributes and relationships from the knowledge graph, 
              enabling precise depictions. The editing stage seamlessly integrates specific items or features into images by retrieving relevant contextual data. 
              Finally, the self-correction process iteratively refines image outputs using RAG context-guided prompts to ensure alignment 
              with intended narratives and visual fidelity, addressing complex character traits and ensuring coherence across 
              cultural and contextual dimensions.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-widescreen">
    <!-- Qualitative Results Main Heading -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h2 class="title is-3">Qualitative Results</h2>
      </div>
    </div>

    <!-- Image Generation Subheading -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h3 class="title is-4">Image Generation</h3>
      </div>
    </div>

    <!-- Image Generation Results -->
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <img id="image-generation" src="./static/images/image-generation.jpg" alt="Image Generation" style="width: 120%; height: auto;">
          <p class="teaser-caption">
          <p style="font-size: 1rem; line-height: 1.5; text-align: justify; margin-top: 10px; margin-bottom: 20px; color: #444;">
            Context Canvas enhances image generation across diverse domains by integrating cultural and contextual details often missed by
            standard models. For example, it accurately portrays rare Indian mythological characters like Tumburu with his horse-face and instrument,
            and Gandabherunda as a dual-headed bird (top left). It also refines depictions of Makara’s hybrid form and Kumbhakarna’s horns (second
            row, left), which Flux omits. Our method adapts to various mythologies, capturing Melinoe’s ghostly essence (left bottom) and Zhong Kui’s
            fierce warrior form (middle bottom). In Project Gutenberg domains, such as Historical Fiction, Gothic Horror, and Fantasy, it captures
            narrative-specific details like Captain Ahab’s ivory leg and gaunt expression (top 4th column) and Edmond Dant`es’ pale skin, coat, and ring
            (top 3rd column). For Gothic Horror, it enhances Count Dracula’s menacing presence (middle 3rd column) and infers Manfred’s guilty,
            dark persona (middle 4th column). Our approach faithfully represents Lilith with bat wings and snakes
            and Lizarel with ethereal beauty and silvery hair (bottom right), demonstrating superior fidelity across cultural and literary domains.
          </p>
        </div>
      </div>
    </div>


    <!-- Self-Correction Subheading -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h3 class="title is-4">Self-Correcting RAG-Guided Diffusion (SRD)</h3>
      </div>
    </div>
    
    <!-- Self-Correction Results -->
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <img id="self-correction" src="./static/images/self-correction.jpg" alt="Self-Correction Results" style="width: 100%; height: auto; margin-top: 20px;">
          <p class="teaser-caption">
          <p style="font-size: 1rem; line-height: 1.5; text-align: justify; margin-top: 10px; margin-bottom: 20px; color: #444;">
            SRD achieves highly accurate and culturally resonant depictions of complex mythological characters
            through iterative, context-rich prompt refinement. SRD corrects Vritra’s multi-headed form, drawing on the story’s cultural narrative 
            from a robust knowledge graph to identify and position Indra accurately. For ‘Garuda’, iterative
            adjustments restore his iconic gold jewelry and vibrant wing color, enhancing his divine representation. For
            “Yama on his vehicle”, our approach automatically identifies his vehicle and adds essential symbols like his crown, mace, and noose, with precise
            skin tone adjustments. Similarly for ‘Mahishasura’, his incorrect tail is transformed into a snake he is typically depicted with.
          </p>
        </div>
      </div>
    </div>


    <!-- Placeholder for Editing and Self-Correction Results -->
    <div class="columns is-centered" id="image-editing" style="margin-top: 30px;">
    <div class="container is-max-desktop">
      <div class="column">
        <!-- Example Placeholder for Editing -->
        <h3 class="title is-4 has-text-centered">Editing</h3>
        <img id="editing-results" src="./static/images/image-editing-copy.jpg" alt="Editing Results" style="width: 100%; height: auto; margin-top: 20px;">
        <h2 class="subtitle result-caption" style="font-size: 1rem; text-align: justify; margin-top: 10px; max-width: 1200px; margin-left: auto; margin-right: auto;">
          Our method enhances ControlNet’s disentangled editing
          by adding culturally accurate elements. In the first row, for Jam-
          bavan, standard ControlNet introduces generic, misaligned items.
          In the second row, our approach accurately depicts ‘his sword’ as
          a fire sword, ‘primary weapon’ as a mace and ‘his daughter’ as
          an adopted human. 
        </h2>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-widescreen">
    <!-- Qualitative Comparison Main Heading -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h2 class="title is-3">Qualitative Comparison</h2>
      </div>
    </div>

    <!-- Comparison Image -->
    <div class="columns is-centered">
      <div class="column">
        <div class="content" style="text-align: justify;">
          <img id="qualitative-comparison" src="./static/images/qualitative-comparison.jpg" alt="Qualitative Comparison" style="width: 100%; height: auto; margin-top: 20px;">
          <p class="teaser-caption">
            This figure highlights the comparative performance of <span class="dnerf">Context Canvas</span> against state-of-the-art methods. Our approach consistently achieves superior results in capturing culturally and contextually accurate details, including intricate features like character-specific appearances and attributes. For example, when depicting Tumburu or Mahishasura, Context Canvas preserves their unique traits with higher fidelity compared to Flux and ControlNet.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-widescreen">
    <!-- Quantitative Results Main Heading -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h2 class="title is-3">Quantitative Results</h2>
      </div>
    </div>

  <!-- Explanation for Qualitative Results -->
  <div class="columns is-centered">
    <div class="column">
      <p style="font-size: 1rem; line-height: 1.5; text-align: justify; margin-top: 20px;">
      We conduct a comprehensive evaluation of 'Context Canvas' in two stages, assessing both its foundational RAG component and 
      image generation. We benchmark our approach against SOTA T2I models-such as Flux, SDXL, and DALL-E 3 across image generation 
      and two rounds of self-correction. 
      </p>
    </div>
  </div>

   <!-- RAG Evaluation Subheading -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h3 class="title is-4">RAG Evaluation</h3>
      </div>
    </div>
    
    <!-- RAG Evaluation Table with Scaled Caption -->
    <div class="columns is-centered">
      <div class="column is-narrow">
        <div class="content" style="text-align: center; width: 30%; margin: 0 auto;">
          <img id="rag-evaluation" src="./static/images/rag-quant-table.jpg" alt="RAG Evaluation Table" style="width: 100%; height: auto; margin-top: 20px;">
          <p class="teaser-caption" style="font-size: 0.9rem; line-height: 1.5; text-align: justify; margin-top: 10px; margin-bottom: 20px;">
            Table 1. Our RAG process achieves high evaluation scores across both retrieval and generation due to meticulous data curation, retrieval, and prompt engineering. 
            Metrics such as Retrieval Accuracy, Context Precision, and Context Recall highlight the effectiveness of our framework .
          </p>
        </div>
      </div>
    </div>


    <!-- Benchmarking Subheading -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h3 class="title is-4">Benchmarking Against SOTA T2I Models</h3>
      </div>
    </div>

    <!-- SOTA Comparison Table -->
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <img id="sota-benchmarking" src="./static/images/image-gen-quant-table.jpg" alt="SOTA Benchmarking Table" style="width: 100%; height: auto; margin-top: 20px;">
          <p class="teaser-caption" style="font-size: 1rem; line-height: 1.5; text-align: justify; margin-top: 10px; margin-bottom: 20px;">
           We compare <span class="dnerf">Context Canvas</span> with state-of-the-art T2I models-such as SDXL, Flux and Dall-E 3, across key quantitative metrics. Our framework achieves superior scores in cultural and contextual fidelity, image coherence, and narrative-specific accuracy, demonstrating its robustness and adaptability across diverse domains.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



    
       






<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{venkatesh2024contextcanvas,
  author    = {Venkatesh, Kavana and Dalva, Yusuf and Lourentzou, Ismini and Yanardag, Pinar },
  title     = {Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge Graph-Based RAG},
  journal   = {arXiv preprint},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website, 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
